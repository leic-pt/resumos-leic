---
title: Avaliação com Utilizadores
description: Avaliação no Ciclo Iterativo
  Métodos de Avaliação.
  Case Study Halo 3.
  Diferentes métodos de Avaliação/Investigação.
  Testes de Usabilidade.
  Testes A/B.
  Efeitos Secundários.

path: /ipm/avaliacao-com-utilizadores
type: content
---

# Avaliação com Utilizadores

```toc

```

## Avaliação no Ciclo Iterativo

Ao realizarmos qualquer tipo de produto, temos que passar por uma série de etapas de modo a podermos obter o melhor protótipo funcional possível. Na figura abaixo conseguimos ver todos os passos que constituem este [**ciclo iterativo**](color:orange). Contudo, agora apenas nos vamos focar nos [**testes**](color:pink).

![Avaliação no Ciclo Iterativo](./assets/0005-avaliacao-no-ciclo-iterativo.png#dark=2)

Para concluir que fizemos um bom trabalho no nosso projeto, temos que realizar [**uma série de testes**](color:pink) com utilizadores exteriores aos membros do nosso grupo para testar a qualidade e utilização no dia-a-dia do que nós fizemos.

Nestes testes pretendemos ter dois objetivos: [**avaliar**](color:orange) e [**identificar problemas**](color:orange), através de duas componentes distintas:

- [**Usabilidade**](color:pink)
- [**Experiência de Utilização**](color:pink)

## Métodos de Avaliação

Contudo, para fazer estes testes temos que nos focar em dois métodos para, como foi referido acima, obtermos as melhores avaliações possiveis e as críticas que geram mais confiança possível. Por isso, realizamos [**testes analíticos**](color:pink) e [**testes empíricos**](color:pink).

### Testes Analíticos

Os teste analíticos referem-se a avaliações heurísticas, a modelos mais preditivos. Isto é, os testes analíticos referem-se somente a como o nosso trabalho está só por si sem haver pessoas a testarem-no. Aos testes analíticos estão associadas a [**Lei de Fitts**](color:pink) e [**Machine Learning**](color:pink).

### Testes Empíricos

Por outro lado, os testes empíricos referem-se a [**testes de usabilidade**](color:orange), [**testes A/B**](color:orange), [**diários**](color:orange), [**shadowing**](color:orange), [**contextual inquiry**](color:orange), entre muitos outros. Estes testes já retratam uma interação com os utilizadores externos para compreender até que ponto o nosso produto pode e é preferido para ser utilizado.

Dentro dos testes empíricos, podemos dividir as observações em duas. Temos [**observações diretas**](color:pink) que retratam utilizadores a serem observados a realizar tarefas, seja presencial ou em vídeo. Um bom exemplo de observação direta é o método [**Wizard of Oz**](color:orange), como foi realizado no primeiro Bake-Off.

Em termos de [**observação indiretas**](color:pink) referimo-nos a dois tipos de interações:

- [**Manuais**](color:orange) - através do uso de diários, por exemplo
- [**Automáticos**](color:orange) - através do registo de interação, isto é, [**interaction logs**](color:pink)

### Como usamos um diário?

Diários podem ser usados para verificar a utilização a longo prazo de um produto, ou seja, a utilização mais regular durante [**uma ou mais semanas**](color:pink) do nosso produto. Os diários permitem ao utilizador externo partilhar os seus pensamentos, a sua utilidade, a relação com o produto a um nível mais elevado que após somente uma utilização. O nosso objetivo é criar um produto que incorpore as atividades do dia-a-dia, e os diários são a melhor forma de avaliar isso mesmo.

## Case Study: Halo 3

### "How Microsoft Labs Invented a New Science of Play"

Com o Halo 3, a Microsoft gerou um [**registo de interação automático**](color:pink), isto é, tinham milhares de jogadores a jogarem todos os dias, avaliando a qualidade e dificuldade excessiva do mesmo.

O que era registado:

- Timestamp
- Localização do jogador
- Número de balas
- Eventos (morrer, acertar, etc.)
- Para onde está a apontar

Ao avaliar as condições, a Microsoft criou um [**Heatmap**](color:orange) que a cada 5s [**mudavam a cor do jogador**](color:pink). Desta forma, se havia muitas cores num só sítio então os programadores sabiam que tinham que [**remodelar o mapa**](color:pink), pois havia muita gente presa nesse sítio, contudo, se o mapa tivesse cores diferentes em lugares diferentes, então tínhamos um [**bom mapa**](color:pink).

Também viam as munições: o [**registo automático**](color:pink) conseguia avaliar se havia uma zona do mapa em que as balas era todas perdidas, contudo, foi necessário o auxílio do [**registo direto**](color:pink) para compreender o [**porquê de perderem tantas balas**](color:pink).

Atualmente, é possível lançar um jogo passados apenas 2 ou 3 anos e ir recebendo feedback à medida que os utilizadores vão experimentando e usufruindo do jogo, havendo uma constante remodelação do jogo, em vez de ser necessário esperar 5 anos para poderem lançar o jogo pela primeira vez. Através do feedback e dos testes, os jogos podem melhorar mas ser logo jogados.

## Diferentes métodos de Avaliação/Investigação

- [**Entrevistas**](color:pink)
- Focus groups
- Questionários online
- Diários
- Sondas culturais
- [**Observação**](color:pink)
- Etnografia
- Experience sampling
- [**Registo de interação**](color:pink)
- Co-desenho
- Casos de estudo
- Contextual inquiry
- [**Wizard of Oz**](color:pink)
- [**Think-aloud**](color:pink)
- Card sorting
- **...**

Nos Bake-Offs passámos por muitos destes métodos.

## Testes de Usabilidade

Para garantir que os nossos produtos são bons e práticos, é necessário fazer testes à usabilidade. Estes testes permitem ao criador [**identificar problemas**](color:pink) (no design e da aplicação ou produto), [**descobrir oportunidades**](color:pink) para melhorar o design e [**compreender os utilizadores**](color:pink) através dos seus comportamentos e preferências.

### Tipos de Testes de Usabilidade

![Tipos de Testes de Usabilidade](./assets/0005-tipos-de-testes-de-usabilidade.png#dark=3)

Para o primeiro bake-off, focámo-nos bastante na [**avaliação formativa**](color:pink), enquanto que no segundo, focámo-nos mais no [**protótipo final**](color:pink) e [**avaliação sumativa**](color:pink). Isto porque não fazia sentido estarmos a fazer avaliações formativas pois ainda não estava feito.

[**Formativa:**](color:orange) realiza-se durante o processo de desenho, os resultados informam a próxima fase de desenho.

- **"O que acontece durante a interação?"**

[**Sumativa:**](color:orange) refere-se a uma avaliação final para aferir sucesso.

- **"Qual o resultado da interação"**, ou seja, conseguimos observar o problema mas não como resolve-lo, é o problema das munições referido acima.

Para realizar um bom teste de usabilidade, temos que focar-nos em três parâmetros (elementos) diferentes:

- Participantes
- Tarefas
- Medidas

### Participantes/Utilizadores

Os participantes devem ser pessoas próximas do público-alvo, com maior disponibilidade. Também temos que ver uma [**sample representativa da população humana**](color:pink), ou seja, temos que ter o mesmo número de homens e mulheres de todas as idades (a não ser, claro, que tenhamos um produto específico para um grupo).

:::tip[NOTA]

Quando nos referimos a [**pessoas**](color:orange) com maior disponibilidade temos que ver pessoas que tenham tempo para avaliar o nosso produto com calma. Por exemplo, se temos um produto para médicos, podemos testá-lo com estudantes de medicina.

:::

Para termos uma boa descrição do nosso grupo de participantes temos que ver a sua [**Demographic Info**](color:orange).

![Demographic Info](./assets/0005-demographic-info.png#dark=3)

Mas quantos utilizadores devemos ter para que o nosso produto seja devidamente testado sem problemas?

Segundo [**Nielsen**](color:orange), devemos disponibilizar [**20 utilizadores**](color:pink): [**4 iterações com 5 utilizadores cada**](color:pink).

Se tivermos

- 1 utilizador &rarr; 33% dos problemas identificados
- 5 utilizadores &rarr; 85% dos problemas identificados
- 15 utilizadores &rarr; 99% dos problemas identificados

segundo um estudo publicado por Nielsen em 1993.

É necessário um grande número de utilizadores para ter a certeza que há sempre uma média não curta e não manipulável.

### Tarefas

As tarefas que apresentamos aos nossos participantes têm que ser bastante bem definidas. Têm que ser:

- [**Reais**](color:pink) e representativas
- [**O quê**](color:pink) e não como
- [**Específicas**](color:pink)
- Mistura de Complexidades
- [**Avaliação Comparativa**](color:pink)
  - não favorecer uma das soluções
  - usar as mesmas tarefas

Por outras palavras, não podemos favorecer uma das soluções, por exemplo, usando ratos melhores ou dizendo comentários do género **"passei vários dias e noites a trabalhar nesta parte"** (o utilizador já sabe que essa parte específica vai ser mais favorável).

### Medidas de Usabilidade

- Tempo para completar a tarefa
- Número de erros cometidos
- Número de tarefas concluídas
- Número de cliques
- Número de consultas à ajuda
- Satisfação do utilizador

### Tipos de dados

- [**Quantitativos**](color:orange) (quantidade, específicos e medíveis) - Segundo Bake-Off

  - [**Completou a tarefa?**](color:pink) Sim/Não
  - [**Quanto tempo demorou?**](color:pink)
  - [**Quantos erros?**](color:pink)
  - [**Qual preferiu?**](color:pink) A ou B

- [**Qualitativo**](color:orange) (qualidade, "aberto") - Primeiro Bake-Off

  - O que gostou mais na sua experiência?
  - O que pensa do ecrã principal?
  - Mais difícil de obter?

- [**Objetivos**](color:orange)

  - Não dependem da pré-disposição (bias) inerente ao ser humano (ex.: tempo, erros, frequência cardíaca, etc.)

- [**Subjetivos**](color:orange)
  - Realça a perceção do utilizador (ex.: preferência, **SUS**, **SEQ**, etc.)

[**SUS:**](color:orange) acima de 68% é minimamente utilizável, já há valores médios por ser tão utilizado.

[**SEQ:**](color:orange) debriefing se tiveram dificuldade a completar alguma tarefa.

### Testes-piloto

Qualquer pessoa pode completá-los, têm que ter 2 a 3 pessoas, é necessário testar procedimento:

- Duração
- Instruções
- Tarefas
- Questionário

:::tip[]

Os testes-piloto ajudam a encontrar últimos erros antes do produto sair. Para a tese de mestrado será necessário fazer isto a demonstrar como funciona.

:::

## Testes A/B

Uma das maneiras mais comuns de testar a usabilidade de um produto é através dos [**testes A/B**](color:pink). Estes testes têm o objetivo de verificar qual a melhor opção de [**cor**](color:orange), [**logótipo**](color:orange), [**layout de página web**](color:orange), [**tipografia**](color:orange), [**botões**](color:orange), etc.

:::tip

Para o segundo bake-off, é muito recomendado fazer estes testes A/B em que apenas fazemos uma alteração ao projeto, para nos facilitar qual o aspeto que temos que mudar. Tal não era recomendado para o primeiro bake-off, em que tinhamos que fazer mais alterações.

:::

:::warning[ATENÇÃO]

É expectável ter que voltar atrás várias vezes e que a versão que é mais favorável seja uma versão mais antiga. Isto irá acontecer especialmente entre os valores 8 e 10.

:::

**_"A/B testing is like flossing! All of us should be doing it but most of us are not."_**

### Google's 41 Shades of Blue

Um bom exemplo de testes A/B é o Google's 41 Shades of Blue: nestes testes, a Google mostrou 41 cores diferentes aos seus utilizadores; sempre que alguém entrava no motor de pesquisa, ia para um [**bucket**](color:orange) e assim a empresa podia ver qual a cor que tinha mais sucesso. Apesar de não ser evidente à priori, uma simples mudança de cor fez com que a empresa ganhasse mais [**$200m por ano**](color:pink).

Esta teste levou a um conflito entre as equipas de Engenharia e de Design na Google, dado a decisão de qual a melhor cor ter sido tomada com base numa experiência prática com utilizadores reais em vez de só com base na _expertise_ da equipa de Design.

### Como dividir os grupos?

Existem duas maneiras de dividir os grupos para fazer os testes:

- [**Intergrupos:**](color:orange) "Between-subjects", é necessário ter em atenção que temos que recrutar o dobro das pessoas e temos que ter em atenção que os grupos têm que ser comparáveis (ex.: um grupo que tem vacina e outro que não).

![Testes Intergrupos](./assets/0005-testes-intergrupos.png#dark=3)

- [**Intragrupos:**](color:orange) "Within-subjects", cada grupo testa as duas interações, vamos trocando a ordem do uso dos sistemas (aprendizagem, fatiga).
  - foi o que fomos fazendo nas aulas práticas durante os bake-offs.

![Testes Intragrupos](./assets/0005-testes-intragrupos.png#dark=3)

Contudo, temos que ter atenção que há uma série de fatores que influenciam as pessoas. Por exemplo, se um grupo já testou 40 vezes o projeto A pode estar mais [**viciado**](color:orange) nesse projeto, ou pode estar mais cansado desse projeto e tem uma opinião mais influenciada.

Então, como lidamos com os efeitos da [**ordem de utilização**](color:pink)? Temos que ir alterando os testes a cada produto, [**counter-balancing**](color:orange).

:::details[Exemplo]

3 sistemas ou versões: A, B, C

Utilizador 1: [**A B C**](color:orange)

Utilizador 2: [**A C B**](color:orange)

Utilizador 3: [**B A C**](color:orange)

Utilizador 4: [**B C A**](color:orange)

Utilizador 5: [**C A B**](color:orange)

Utilizador 6: [**C B A**](color:orange)

:::

Counter-balancing funciona de uma forma fatorial, resumidamente:

- 3 sistemas: 3! = 6 utilizadores
- 4 sistemas: 4! = 24 utilizadores
- 5 sistemas: 5! = 120 utilizadores

### Variáveis Dependentes e Independentes

[**Variáveis Dependentes:**](color:orange)

- O seu valor depende do sistema a testar
- Variáveis medidas no estudo (tempo, erros, SUS)
- Relacionados com o objetivo do protótipo

[**Variáveis Independentes:**](color:orange)

- Não dependem das variáveis que estamos a medir
- Características da solução (layout, cor, etc.)
- Características dos participantes (idade, etc.)

[**Exemplo do segundo bake-off:**](color:orange)

- [**Teste sumativo**](color:pink)
- [**Intragrupos (se testado nos labs)**](color:pink)
- [**Variáveis dependentes**](color:pink)
  - Tempo, taxa de erro
- [**Variáveis independentes**](color:pink)
  - Solução mais recente do grupo x (esquema de cores, representação do alvo a escolher, etc.)
- [**Dados quantitativos**](color:pink)

## Efeitos Secundários

Deparamo-nos agora com um problema, um problema de [**fixação funcional**](color:orange), feito por Duncker em 1945. Duncker colocou à frente dos seus participantes uma vela, uma caixa de pioneses e fósforos e pediu para cada pessoa arranjar uma forma para que a vela fosse acendida mas não pingasse cera para a mesa. Contudo, dividiu os participantes em dois grupos:

- Grupo A tinha os pioneses dentro da caixa
- Grupo B tinha os pioneses fora da caixa

Como os pioneses estavam fora da caixa era mais evidente para o grupo B que a caixa trabalhava como outro elemento que pudesse ser utilizado e não somente como um suporte para os pioneses. Assim, o grupo B acabou mais rapidamente a experiência.

Em 1962, Glucksberg testou a mesma experiência, mas adicionou mais uma variável:

- Obter o tempo médio de resolução
- Recompensa pelo tempo mais rápido. Top 25% recebe \$40, mais rápido recebe \$150.

![Desenho Experimental 2x2](./assets/0005-desenho-experimental-2x2.png#dark=3)

Ao motivar os participantes, no grupo que tinha os pioneses na caixa, ou seja, os participantes que necessitavam de [**mais criatividade**](color:orange), o grupo com recompensa foi mais lento, enquanto no grupo que tinha os pioneses dentro da caixa, o grupo com recompensa foi mais rápido.

Associar recompensa a desempenho aumentou o foco mas restringe a criatividade, é bom para tarefas simples com instruções claras.

### Efeito de Hawthorne / Efeito do Observador

**_"It was suggested that the productivity gain occurred as a result of the motivational effect on the workers of the interest being shown in them."_** - Henry A. Landsborg, 1958

[**Por outras palavras, quando há mais pressão, há um ganho de produtividade.**](color:pink)

[**Novelty effect:**](color:orange) quando vemos algo novo, ficamos mais excitados sem razão. O produto até pode não ser nada de especial, mas, só por ser algo novo, temos curiosidade. Por isso, temos que testar o produto mais do que uma vez para testar o quão bom e prático é.
